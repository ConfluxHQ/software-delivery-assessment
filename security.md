# Security and Securability check

> **Part of the Multi-team Software Delivery Assessment** ([README](README.md))
> 
> Copyright © 2018-2021 [Conflux Digital Ltd](https://confluxdigital.net/)
> 
> Licenced under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) ![CC BY-SA 4.0](https://licensebuttons.net/l/by-sa/3.0/88x31.png)
>
> _Permalink: [SoftwareDeliveryAssessment.com](http://SoftwareDeliveryAssessment.com/)_ 

Based on selected criteria from the following books:

* [_Agile Application Security_](https://www.oreilly.com/library/view/agile-application-security/9781491938836/) by Laura Bell, Michael Brunton-Spall, Rich Smith, Jim Bird
* [_Alice and Bob Learn Application Security_](https://www.wiley.com/en-gb/Alice+and+Bob+Learn+Application+Security-p-9781119687405) by Tanya Janca
* [_Secure by Design_](https://www.manning.com/books/secure-by-design) by Dan Bergh Johnsson, Daniel Deogun, Daniel Sawano
* [Continuous Delivery](http://continuousdelivery.com/) by Jez Humble and Dave Farley
* [Thread Modeling: Designing for Security](https://shostack.org/books/threat-modeling-book) by Adam Shostack

The [OWASP Top Ten](https://owasp.org/www-project-top-ten/) list of application security risks is an excellent starting point for assessing security and securability in software. The game [Elevation of Privilege](https://www.microsoft.com/security/blog/2010/03/02/announcing-elevation-of-privilege-the-threat-modeling-game/) by Adam Shostack was also an influence, especially in the [card deck format provided by Agile Stationery](https://agilestationery.com/collections/cybersecurity-games/products/elevation-of-privilege-game).

<a href="https://agilestationery.com/collections/cybersecurity-games/products/elevation-of-privilege-game" title="Elevation of Privilege game card deck from Agile Stationery"><img src="images/EOP_deck_2000x.jpg" alt="Photo of Elevation of Privilege game card deck from Agile Stationery" width="200px" /></a>

Purpose: *Assess the approach to security and securability practices within the software system.* 

Method: Use the [*Spotify Squad Health Check*](https://labs.spotify.com/2014/09/16/squad-health-check-model/) approach to assess the team's answers to the following questions, and also capture the answers:

| **Question**                                                                                                                                                                           | **Tired (1)**                                                                    | **Inspired (5)**                                                                                                                                                                                                                     |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1\. **OWASP Top Ten** - Do you check for the the [OWASP Top Ten](https://owasp.org/www-project-top-ten/) security risks?                                                                                   | We check for some of the OWASP Top Ten occasionally or manually.                                         | We have automated checks for the OWASP Top Ten that run on every commit/check-in to our deployment pipeline.                                                                                                                                                                                  |
| 2\. **Secure Design Principles** - What is the approach to security and compliance?                                                                | Compliance with a defined set of criteria is more important than taking the time to do good design.                            | [We value a culture of finding and fixing design issues over checkbox compliance.](https://www.threatmodelingmanifesto.org/#values)                                                                                                                                                 |
| 3\. **Threat Modeling** - How do you approach threat modeling?                                                                    | We build a system and put changes into production. Later,  we might ask an expert outside the team to try and break it.                           | We model our system and review it for threats whenever we are performing design work. We use the [four questions frame](https://github.com/adamshostack/4QuestionFrame) [1) What are we working on? 2) What can go wrong? 3) What are we going to do about it? 4) Did we do a good job?] to provide structure and repeatability to the process.                                                                                                                                                 |
| 4\. **Domain-driven Security** - In what ways to you work with domain experts to help make code secure?                                                    | We use domain experts to define software features but not secure code.                                       | We use the expertise and viewpoints of domain experts to help us model the domain accurately with deep modeling and domain invariants. This leads to code that is inherently more secure because it uses more specific data types, not just the primitives like `String`, `integer`, etc.                                                                                                                                                                                          |
| 5\. **Input Testing** - What kind of input testing do you perform in the deployment pipeline?                                                          | We do not test the inputs into the software application **or** we rely on client-side validation only.                   | We perform at least four types of input testing: Normal input (input that confirms to domain rules), Boundary input (input that conforms only to structural correctness), Invalid input (input like `null` and strange characters), and Extreme input (such as 40 million characters).                                                                                                                                                                 |
| 6\. **SLIs** - What combination of three or four **indicators or metrics** do you use (or could/would you use) to provide a **comprehensive picture of the health and availability** of your software in production/live?                                                       | We don't have a set of key metrics for the health and availability of the application/service.                                | We have a clear, agreed set of key metrics for each application/service and we display this figure on a team-visible dashboard. The dashboard data is updated at least every 10 minutes.                                                                                                                               |
| 7\. **Error Budget and similar mechanisms** - How does the team know when to **spend time on operational aspects** of the software (logging, metrics, performance, reliability, security, etc.)? Does that time actually get spent?                                                    | We spend time on operational aspects only when there is a problem that needs fixing.                                     | We allocate between 20% and 30% of our time for working on operational aspects and we check this each week. We alert if we have not spent time on operational aspects --OR-- We use SRE Error Budgets to plan our time spent on operational aspects.                                                                              |
| 8\. **Alerting** - What proportion (approximately) of your time and effort as a team do you spend on **making alerts and operational messages more reliable and more relevant**?                                                                                       | We spend as little time as possible on alerts and operational messages - we need to focus on user-visible features.                                | We regularly spend time reviewing and improving alerts and operational messages.                                                                                                                                                      |
| 9\. **Toil and fixing problems** - What proportion (approx) of your time gets taken up with incidents from live systems and how predictable is the time needed to fix problems?                                    | We do not deal with issues from live systems at all - we focus on new features --OR-- live issues can really affect our delivery cadence and are very disruptive.                     | We allocate a consistent amount of time for dealing with live issues --OR-- one team member is responsible for triage of live issues each week OR we rarely have problems with live issues because the software works well. | 
| 10\. **Time to Diagnose** - How long does it typically take to diagnose problems in the live/production environment? This is the time taken to understand and pinpoint what is wrong (not to fix or remediate the problem).                                    | It can take hours or days to diagnose most problems in live/production.                     | It typically takes seconds or minutes to diagnose most problems in live/production. | 


